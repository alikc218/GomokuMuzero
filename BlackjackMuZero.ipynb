{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMdecL9O4VRXaXQFQQEhXXp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alikc218/GomokuMuzero/blob/main/BlackjackMuZero.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.21.0 torch tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "vfpBhGKEStls",
        "outputId": "ef5d07c9-79fa-4668-9307-f2afa68ccb50"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.21.0\n",
            "  Using cached numpy-1.21.0.zip (10.3 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m105.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m95.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: numpy\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for numpy \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for numpy (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31m  ERROR: Failed building wheel for numpy\u001b[0m\u001b[31m\n",
            "\u001b[0mFailed to build numpy\n",
            "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (numpy)\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w-4AziuGa4v3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from enum import Enum, auto\n",
        "\n",
        "class BlackjackAction(Enum):\n",
        "    HIT = 0\n",
        "    STAND = 1\n",
        "    DOUBLE = 2\n",
        "    SPLIT = 3\n",
        "\n",
        "class BlackjackEnv:\n",
        "    def __init__(self, decks=6):\n",
        "        self.decks = decks\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.deck = self._create_deck()\n",
        "        np.random.shuffle(self.deck)\n",
        "        self.deck_pos = 0\n",
        "        self.player_hand = [self._draw_card(), self._draw_card()]\n",
        "        self.dealer_hand = [self._draw_card(), self._draw_card()]\n",
        "        self.done = False\n",
        "        return self._get_obs()\n",
        "\n",
        "    def _create_deck(self):\n",
        "        return np.array([2,3,4,5,6,7,8,9,10,10,10,10,11]*4*self.decks)\n",
        "\n",
        "    def _draw_card(self):\n",
        "        card = self.deck[self.deck_pos]\n",
        "        self.deck_pos += 1\n",
        "        return int(card)\n",
        "\n",
        "    def _get_obs(self):\n",
        "        return {\n",
        "            'player_sum': self._hand_value(self.player_hand),\n",
        "            'dealer_card': self.dealer_hand[0],\n",
        "            'usable_ace': self._has_usable_ace(self.player_hand),\n",
        "            'can_split': len(self.player_hand) == 2 and self.player_hand[0] == self.player_hand[1]\n",
        "        }\n",
        "\n",
        "    def _hand_value(self, hand):\n",
        "        total = sum(hand)\n",
        "        aces = hand.count(11)\n",
        "        while total > 21 and aces:\n",
        "            total -= 10\n",
        "            aces -= 1\n",
        "        return total\n",
        "\n",
        "    def _has_usable_ace(self, hand):\n",
        "        return 11 in hand and self._hand_value(hand) <= 21\n",
        "\n",
        "    def step(self, action):\n",
        "        if self.done:\n",
        "            raise ValueError(\"Game ended\")\n",
        "\n",
        "        reward = 0\n",
        "        if action == BlackjackAction.HIT:\n",
        "            self.player_hand.append(self._draw_card())\n",
        "            if self._hand_value(self.player_hand) > 21:\n",
        "                self.done = True\n",
        "                reward = -1\n",
        "        elif action == BlackjackAction.STAND:\n",
        "            self._dealer_play()\n",
        "            reward = self._get_result()\n",
        "            self.done = True\n",
        "\n",
        "        return self._get_obs(), reward, self.done, {}\n",
        "\n",
        "    def _dealer_play(self):\n",
        "        while self._hand_value(self.dealer_hand) < 17:\n",
        "            self.dealer_hand.append(self._draw_card())\n",
        "\n",
        "    def _get_result(self):\n",
        "        player = self._hand_value(self.player_hand)\n",
        "        dealer = self._hand_value(self.dealer_hand)\n",
        "\n",
        "        if player > 21: return -1\n",
        "        if dealer > 21: return 1\n",
        "        if player > dealer: return 1\n",
        "        if player < dealer: return -1\n",
        "        return 0\n",
        "\n",
        "    def legal_actions(self):\n",
        "        return [a for a in BlackjackAction]"
      ],
      "metadata": {
        "id": "UCnMvYbudtcg"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class BlackjackMuZero(nn.Module):\n",
        "    def __init__(self, hidden_size=64):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Representation network\n",
        "        self.rep_net = nn.Sequential(\n",
        "            nn.Linear(4, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Dynamics network\n",
        "        self.dyn_net = nn.Sequential(\n",
        "            nn.Linear(hidden_size + 4, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Prediction network\n",
        "        self.policy_net = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, 4),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "\n",
        "        self.value_net = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, 1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def _prepare_observation(self, obs):\n",
        "        \"\"\"Convert observation dict to tensor\"\"\"\n",
        "        if isinstance(obs, dict):\n",
        "            return torch.FloatTensor([\n",
        "                obs['player_sum'] / 21.0,\n",
        "                obs['dealer_card'] / 11.0,\n",
        "                float(obs['usable_ace']),\n",
        "                float(obs['can_split'])\n",
        "            ]).unsqueeze(0)  # Add batch dimension\n",
        "        return obs\n",
        "\n",
        "    def _prepare_action(self, action):\n",
        "        \"\"\"Convert action to one-hot tensor\"\"\"\n",
        "        action_onehot = torch.zeros(4)\n",
        "        action_onehot[action] = 1.0\n",
        "        return action_onehot.unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "    def representation(self, obs):\n",
        "        \"\"\"Initial state representation\"\"\"\n",
        "        x = self._prepare_observation(obs)\n",
        "        return self.rep_net(x)\n",
        "\n",
        "    def dynamics(self, state, action):\n",
        "        \"\"\"State transition function\"\"\"\n",
        "        action_encoded = self._prepare_action(action)\n",
        "        x = torch.cat([state, action_encoded], dim=1)\n",
        "        return self.dyn_net(x)\n",
        "\n",
        "    def prediction(self, state):\n",
        "        \"\"\"Policy and value prediction\"\"\"\n",
        "        return self.policy_net(state), self.value_net(state)\n",
        "\n",
        "    def initial_inference(self, obs):\n",
        "        \"\"\"Initial pass through the network\"\"\"\n",
        "        state = self.representation(obs)\n",
        "        policy, value = self.prediction(state)\n",
        "        return {\n",
        "            'state': state,\n",
        "            'policy': policy,\n",
        "            'value': value\n",
        "        }\n",
        "\n",
        "    def recurrent_inference(self, state, action):\n",
        "        \"\"\"Recurrent pass through the network\"\"\"\n",
        "        next_state = self.dynamics(state, action)\n",
        "        policy, value = self.prediction(next_state)\n",
        "        return {\n",
        "            'state': next_state,\n",
        "            'policy': policy,\n",
        "            'value': value\n",
        "        }"
      ],
      "metadata": {
        "id": "nb-mUChddvAh"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "class Node:\n",
        "    def __init__(self, prior):\n",
        "        self.visit_count = 0\n",
        "        self.value_sum = 0\n",
        "        self.prior = prior\n",
        "        self.children = {}  # Словарь для дочерних узлов\n",
        "        self.state = None\n",
        "        self.reward = 0\n",
        "\n",
        "    def expanded(self):\n",
        "        \"\"\"Проверяет, раскрыт ли узел (есть ли дети)\"\"\"\n",
        "        return len(self.children) > 0\n",
        "\n",
        "    def value(self):\n",
        "        \"\"\"Возвращает среднее значение узла\"\"\"\n",
        "        if self.visit_count == 0:\n",
        "            return 0\n",
        "        return self.value_sum / self.visit_count\n"
      ],
      "metadata": {
        "id": "8ot9Xg1Rd0hh"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yKLUcFHlhBZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MCTS:\n",
        "    def __init__(self, model, num_simulations=50, c_puct=1.0):\n",
        "        self.model = model\n",
        "        self.num_simulations = num_simulations\n",
        "        self.c_puct = c_puct\n",
        "\n",
        "    def run(self, observation):\n",
        "        root = Node(0)\n",
        "        root.state = self.model.representation(observation)\n",
        "\n",
        "        for _ in range(self.num_simulations):\n",
        "            node = root\n",
        "            search_path = [node]\n",
        "            action = 0  # Действие по умолчанию\n",
        "\n",
        "            # Фаза Selection (выбор до листа)\n",
        "            while node.expanded():\n",
        "                action, node = self.select_child(node)\n",
        "                search_path.append(node)\n",
        "\n",
        "            # Фаза Expansion (раскрытие узла)\n",
        "            parent = search_path[-2] if len(search_path) >= 2 else root\n",
        "\n",
        "            if node.visit_count > 0 and not node.expanded():\n",
        "                # Получаем политику из модели\n",
        "                policy, _ = self.model.prediction(node.state)\n",
        "                policy = policy.squeeze(0).detach().numpy()\n",
        "\n",
        "                # Создаем дочерние узлы\n",
        "                for a, prob in enumerate(policy):\n",
        "                    node.children[a] = Node(prob)\n",
        "\n",
        "                # Выбираем действие согласно политике\n",
        "                action = np.random.choice(len(policy), p=policy)\n",
        "                node = node.children[action]\n",
        "                search_path.append(node)\n",
        "\n",
        "            # Фаза Simulation (прогон динамики)\n",
        "            with torch.no_grad():\n",
        "                # Убедимся, что состояние родителя существует\n",
        "                if parent.state is None:\n",
        "                    parent.state = self.model.representation(observation)\n",
        "\n",
        "                next_state = self.model.dynamics(parent.state, action)\n",
        "                reward = 0\n",
        "                policy, value = self.model.prediction(next_state)\n",
        "\n",
        "                # Обновляем узел\n",
        "                node.state = next_state\n",
        "                node.reward = reward\n",
        "\n",
        "            # Фаза Backpropagation (обновление статистик)\n",
        "            self.backpropagate(search_path, value.item(), reward)\n",
        "\n",
        "        # Возвращаем вероятности действий\n",
        "        visit_counts = np.array([\n",
        "            root.children[a].visit_count\n",
        "            for a in range(4) if a in root.children\n",
        "        ])\n",
        "\n",
        "        if len(visit_counts) == 0:\n",
        "            return np.ones(4)/4  # Равномерное распределение если нет посещений\n",
        "\n",
        "        return visit_counts / visit_counts.sum()\n",
        "\n",
        "    def select_child(self, node):\n",
        "        \"\"\"Выбор дочернего узла по UCB\"\"\"\n",
        "        total_visits = sum(c.visit_count for c in node.children.values())\n",
        "\n",
        "        def ucb_score(child):\n",
        "            if child.visit_count == 0:\n",
        "                return float('inf')  # Всегда исследуем непосещенные узлы\n",
        "            return child.value() + self.c_puct * child.prior * math.sqrt(total_visits) / (child.visit_count + 1)\n",
        "\n",
        "        return max(node.children.items(), key=lambda item: ucb_score(item[1]))\n",
        "\n",
        "    def backpropagate(self, path, value, reward):\n",
        "        \"\"\"Обновление статистик вдоль пути\"\"\"\n",
        "        for node in reversed(path):\n",
        "            node.value_sum += value\n",
        "            node.visit_count += 1\n",
        "            value = reward + 0.99 * value  # С учетом discount factor"
      ],
      "metadata": {
        "id": "2J_QFIauhC8s"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import deque\n",
        "import random\n",
        "import torch.optim as optim\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, model, lr=1e-3, buffer_size=10000, batch_size=32):\n",
        "        self.model = model\n",
        "        self.optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "        self.replay_buffer = deque(maxlen=buffer_size)\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def self_play(self, env, num_games=10):\n",
        "        for _ in range(num_games):\n",
        "            obs = env.reset()\n",
        "            done = False\n",
        "            trajectory = []\n",
        "\n",
        "            while not done:\n",
        "                action_probs = MCTS(self.model).run(obs)\n",
        "                action = np.random.choice(4, p=action_probs)\n",
        "                next_obs, reward, done, _ = env.step(action)\n",
        "\n",
        "                trajectory.append({\n",
        "                    'obs': obs,\n",
        "                    'action_probs': action_probs,\n",
        "                    'reward': reward,\n",
        "                    'done': done\n",
        "                })\n",
        "                obs = next_obs\n",
        "\n",
        "            # Add value targets\n",
        "            value_target = 0\n",
        "            for t in reversed(trajectory):\n",
        "                value_target = t['reward'] + 0.99 * value_target * (1 - t['done'])\n",
        "                t['value_target'] = value_target\n",
        "\n",
        "            self.replay_buffer.extend(trajectory)\n",
        "\n",
        "    def train_step(self):\n",
        "        if len(self.replay_buffer) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        batch = random.sample(self.replay_buffer, self.batch_size)\n",
        "        sample = batch[0]  # Using single sample for simplicity\n",
        "\n",
        "        # Forward pass\n",
        "        output = self.model.initial_inference(sample['obs'])\n",
        "\n",
        "        # Compute losses\n",
        "        policy_loss = -torch.mean(\n",
        "            torch.sum(torch.log(output['policy']) *\n",
        "            torch.FloatTensor(sample['action_probs']))\n",
        "        )\n",
        "        value_loss = F.mse_loss(\n",
        "            output['value'].squeeze(),\n",
        "            torch.FloatTensor([sample['value_target']])\n",
        "        )\n",
        "        total_loss = policy_loss + value_loss\n",
        "\n",
        "        # Backward pass\n",
        "        self.optimizer.zero_grad()\n",
        "        total_loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return total_loss.item()"
      ],
      "metadata": {
        "id": "b4AlPu-3eI4j"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "def evaluate(model, env, num_games=20):\n",
        "    wins = 0\n",
        "    for _ in range(num_games):\n",
        "        obs = env.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action_probs = MCTS(model, num_simulations=20).run(obs)\n",
        "            action = np.argmax(action_probs)\n",
        "            obs, reward, done, _ = env.step(action)\n",
        "\n",
        "        if reward > 0:\n",
        "            wins += 1\n",
        "\n",
        "    return wins / num_games\n",
        "\n",
        "def main():\n",
        "    # Initialize\n",
        "    env = BlackjackEnv()\n",
        "    model = BlackjackMuZero()\n",
        "    trainer = Trainer(model)\n",
        "\n",
        "    # Training loop\n",
        "    num_episodes = 100\n",
        "    for episode in tqdm(range(num_episodes)):\n",
        "        trainer.self_play(env, num_games=5)\n",
        "\n",
        "        # Train on collected data\n",
        "        for _ in range(10):\n",
        "            loss = trainer.train_step()\n",
        "\n",
        "        # Evaluation\n",
        "        if episode % 10 == 0:\n",
        "            eval_score = evaluate(model, env)\n",
        "            print(f\"Episode {episode}, Loss: {loss:.3f}, Win rate: {eval_score:.2f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Test model initialization\n",
        "    env = BlackjackEnv()\n",
        "    model = BlackjackMuZero()\n",
        "\n",
        "    test_obs = env.reset()\n",
        "    print(\"Test observation:\", test_obs)\n",
        "\n",
        "    out = model.initial_inference(test_obs)\n",
        "    print(\"Initial inference:\")\n",
        "    print(\"State shape:\", out['state'].shape)\n",
        "    print(\"Policy shape:\", out['policy'].shape)\n",
        "    print(\"Value shape:\", out['value'].shape)\n",
        "\n",
        "    # Start training\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4U_3mtrpePgU",
        "outputId": "21ad7f74-357f-471c-bf35-a3a735b128fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test observation: {'player_sum': 16, 'dealer_card': 2, 'usable_ace': False, 'can_split': False}\n",
            "Initial inference:\n",
            "State shape: torch.Size([1, 64])\n",
            "Policy shape: torch.Size([1, 4])\n",
            "Value shape: torch.Size([1, 1])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/100 [00:00<?, ?it/s]"
          ]
        }
      ]
    }
  ]
}